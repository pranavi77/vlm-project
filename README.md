

Vision-Language Project (Images & Video)

This repository explores how modern vision-language models (VLMs) can be used to understand images and videos through natural language.

The project is intentionally simple and educational. It focuses on how these models behave in practice, what they are good at, and where their limitations are, rather than trying to build a production system.

⸻

What’s in this repository

The project is organized into two independent parts.

Image understanding (src/image/)

This module demonstrates image retrieval and similarity search.

Images are encoded into embeddings using a vision encoder. These embeddings can then be searched using text queries to retrieve semantically similar images.

This pattern is commonly used in:
	•	Image search
	•	Visual similarity systems
	•	Retrieval-augmented generation (RAG) pipelines

In simple terms, images and text are mapped into a shared space where similar concepts end up close to each other.

⸻

Video understanding (src/video/)

This module demonstrates video question answering using a video-capable vision-language model.

Instead of processing every frame, the system:
	•	Samples a small number of representative frames
	•	Feeds those frames along with a question to the model
	•	Generates a natural-language answer

This allows the model to reason about scenes, actions, and events rather than low-level visual details.

⸻

What this project does well

This project works best for semantic understanding, such as:
	•	Describing what is happening in a video
	•	Identifying actions or events
	•	Understanding scene context
	•	Answering high-level questions about images or videos

Example questions:
	•	What is happening in the video?
	•	Is the vehicle slowing down or stopping?
	•	Are pedestrians visible?
	•	Is this a city street or a highway?

⸻

What this project is not meant for

This project is not designed for:
	•	Exact counting (for example, number of cars)
	•	Precise measurements such as speed or distance
	•	Frame-accurate or pixel-level analysis
	•	Long videos without sampling or chunking

Vision-language models operate on compressed visual representations, not raw video streams.
They reason approximately, similar to how humans describe scenes, rather than how detectors measure them.

For tasks like counting or tracking, a traditional computer vision detector is the correct tool.

⸻

Requirements

This project was developed and tested with Python 3.11.

Python dependencies:
	•	torch
	•	torchvision
	•	torchaudio
	•	transformers
	•	accelerate
	•	ffmpeg-python
	•	pillow
	•	faiss-cpu
	•	datasets

System dependency:
	•	FFmpeg (required for video frame extraction)

⸻

Environment setup

Create and activate a Python 3.11 environment:

conda create -n vlm312_py311 python=3.11 -y
conda activate vlm312_py311

Upgrade pip:

pip install –upgrade pip

Install Python dependencies:

pip install torch torchvision torchaudio
pip install transformers accelerate pillow ffmpeg-python
pip install faiss-cpu datasets

Install FFmpeg on macOS:

brew install ffmpeg

⸻

Running the video QA demo

python src/video/video_qa.py path/to/video.mp4

After the model loads, you can type questions at the prompt:

Question> what is happening in the video?
Question> is it daytime or nighttime?
Question> are pedestrians visible?
exit

⸻

Repository notes

To keep the repository clean and lightweight:
	•	No datasets or videos are committed
	•	Generated embeddings and search results are ignored
	•	Only source code is tracked

Ignored artifacts include:
	•	data/
	•	embeddings/
	•	search_results*/
	•	*.mp4
	•	*.zip

⸻

Design philosophy

The project separates:
	•	Perception and retrieval (image embeddings)
	•	Reasoning and explanation (vision-language models)

This mirrors real-world systems where specialized models extract structured information and language models explain, summarize, or reason about that information.

⸻

Why this project exists

This repository exists to demonstrate practical usage of vision-language models, clearly show their strengths and limitations, and serve as a clean foundation for demos, experiments, or future extensions.

It is a learning-first project: simple, focused, and honest about what the models can and cannot do.

⸻

If you want, next we can:
	•	tighten this for a recruiter-facing repo
	•	add a demo section with screenshots
	•	or write a one-paragraph project summary for your resume
